# Title: Upper bounds on model performance

## Keywords
deep learning, mechanistic interpretability

## TL;DR
We try to understand the inner workings of machine learning models.

## Abstract
In the past 10 years, the rate of growth of machine learning is scaling at unprecendented rates. Day by day, we deal with larger models and larger amounts of data. However, we have yet to develop a strong understanding of how these models work. Much of this come from the infeasibility of us humans understanding the high dimensional spaces that these models operate within. When neural networks become so wide and so deep, their representation abilities become so strong that we simply cannot comprehend them. However, there have been significant inroads into the inner thoughts of the models, through mechanistic interpretability. We have developed many tools, from simple dimensionality reduction methods like PCA or tSNE to more complex systems like sparse autoencoders to help us understand and visualize what models are doing. In this research, we want to improve our understanding of model working through the following paradigms:

1. Ablation studies. Ablate and hyperparameter tune the settings of some recent research result to find out what ingredients in their recipe really make things work.

2. Develop new interpretability tools or modify existing tools and run some test on existing models to help us understand what are the key properties within the model.

3. Come up with good visualizations to show what we previously did not know.

Think out of the box to what you can do, as an AI researcher with the ability to run many experiments in parallel to figure out what works and doesn't, to better our understanding of machine learning as a whole. 