# Title: Upper bounds on model performance

## Keywords
deep learning, statistics

## TL;DR
We try to derive upper bounds on training and test loss performances of classic deep learning models

## Abstract
The speed of progress in deep learning within the past 10 years has almost undoubtedly been one of the fastest in the history of scientific research. While much of this has gone to improve human lives through creating more intelligent systems, much of the strengths of deep neural networks remain shallowly understood. We have developed some rudimentary methods of understanding simple neural networks (universal approximation) and some intuitions of scaling to higher dimensions (Johnson-Lindestrauss). We would like to explore how we can extend existing mathematical tools or develop new methods to understand neural networks. Examples include, but are not limited to, formalizing the strengths and weaknesses of convolutional neural networks, understanding what makes the attention mechanism so ubiquitous, etc. 